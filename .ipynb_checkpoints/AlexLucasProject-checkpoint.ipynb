{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#our dataset: http://pascal.inrialpes.fr/data/human/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 faces!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load in image and detect face(s), https://realpython.com/face-recognition-with-python/ (need adjust size of squares to include hair)\n",
    "\n",
    "# Create the haar cascade\n",
    "faceCascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "TEST_PHOTO = \"../OtherFinalProjectStuff/INRIAPerson/Train/pos/person_139.png\"\n",
    "\n",
    "# Read the image\n",
    "image = cv2.imread(TEST_PHOTO)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect faces in the image\n",
    "faces = faceCascade.detectMultiScale(\n",
    "    gray,\n",
    "    scaleFactor=1.1,\n",
    "    minNeighbors=3,\n",
    "    minSize=(15, 15),\n",
    "    flags = cv2.CASCADE_SCALE_IMAGE #https://stackoverflow.com/questions/41341409/where-is-cv-haar-scale-image-in-opencv-3-1-0-with-python-3-5\n",
    ")\n",
    "\n",
    "print(\"Found {0} faces!\".format(len(faces)))\n",
    "\n",
    "# Draw a rectangle around the faces\n",
    "for (x, y, w, h) in faces:\n",
    "    cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "cv2.imshow(\"Faces found\", image)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87 133 24 24\n",
      "300 130 32 32\n",
      "211 160 30 30\n"
     ]
    }
   ],
   "source": [
    "for (x,y,w,h) in faces:\n",
    "    print(x, y, w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#faces[0][0] = 442\n",
    "#faces[0][1] = 353\n",
    "#faces[0][2] = 50\n",
    "#faces[0][3] = 50\n",
    "\n",
    "#import numpy as np\n",
    "#faces = np.delete(faces, 0, axis=0)\n",
    "#print(faces)\n",
    "\n",
    "#faces[0][2] = 32\n",
    "#faces[0][3] = 32\n",
    "#faces[0][0] = 226\n",
    "#faces[0][1] = 94\n",
    "\n",
    "#faces = np.delete(faces, 0, axis=0)\n",
    "\n",
    "#print(faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image = cv2.imread(TEST_PHOTO)\n",
    "\n",
    "#Need hardcoded offsets for this! For the kid, its +10, -5\n",
    "\n",
    "#for (x, y, w, h) in faces:\n",
    "#    cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "#cv2.imshow(\"Faces found\", image)\n",
    "#cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(TEST_PHOTO)\n",
    "\n",
    "index = 1\n",
    "for (x, y, w, h) in faces:\n",
    "    cv2.imwrite('roi' + str(index) + '.png', image[y:y+h, x:x+w]) #https://stackoverflow.com/questions/9084609/how-to-copy-a-image-region-using-opencv-in-python\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import Add, Conv2D, Input, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "DIV2K_RGB_MEAN = np.array([0.4488, 0.4371, 0.4040]) * 255\n",
    "\n",
    "def edsr(scale, num_filters=64, num_res_blocks=8, res_block_scaling=None):\n",
    "    \"\"\"Creates an EDSR model.\"\"\"\n",
    "    x_in = Input(shape=(None, None, 3))\n",
    "    x = Lambda(normalize)(x_in)\n",
    "\n",
    "    x = b = Conv2D(num_filters, 3, padding='same')(x)\n",
    "    for i in range(num_res_blocks):\n",
    "        b = res_block(b, num_filters, res_block_scaling)\n",
    "    b = Conv2D(num_filters, 3, padding='same')(b)\n",
    "    x = Add()([x, b])\n",
    "\n",
    "    x = upsample(x, scale, num_filters)\n",
    "    x = Conv2D(3, 3, padding='same')(x)\n",
    "\n",
    "    x = Lambda(denormalize)(x)\n",
    "    return Model(x_in, x, name=\"edsr\")\n",
    "\n",
    "\n",
    "def res_block(x_in, filters, scaling):\n",
    "    \"\"\"Creates an EDSR residual block.\"\"\"\n",
    "    x = Conv2D(filters, 3, padding='same', activation='relu')(x_in)\n",
    "    x = Conv2D(filters, 3, padding='same')(x)\n",
    "    if scaling:\n",
    "        x = Lambda(lambda t: t * scaling)(x)\n",
    "    x = Add()([x_in, x])\n",
    "    return x\n",
    "\n",
    "\n",
    "def upsample(x, scale, num_filters):\n",
    "    def upsample_1(x, factor, **kwargs):\n",
    "        \"\"\"Sub-pixel convolution.\"\"\"\n",
    "        x = Conv2D(num_filters * (factor ** 2), 3, padding='same', **kwargs)(x)\n",
    "        return Lambda(pixel_shuffle(scale=factor))(x)\n",
    "\n",
    "    if scale == 2:\n",
    "        x = upsample_1(x, 2, name='conv2d_1_scale_2')\n",
    "    elif scale == 3:\n",
    "        x = upsample_1(x, 3, name='conv2d_1_scale_3')\n",
    "    elif scale == 4:\n",
    "        x = upsample_1(x, 2, name='conv2d_1_scale_2')\n",
    "        x = upsample_1(x, 2, name='conv2d_2_scale_2')\n",
    "\n",
    "    return x\n",
    "\n",
    "def pixel_shuffle(scale):\n",
    "    return lambda x: tf.nn.depth_to_space(x, scale)\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    return (x - DIV2K_RGB_MEAN) / 127.5\n",
    "\n",
    "\n",
    "def denormalize(x):\n",
    "    return x * 127.5 + DIV2K_RGB_MEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import PiecewiseConstantDecay\n",
    "#send face(s) through super resolution network, (????? TODO, train on Colab, load here, find LR/SR face dataset as well)\n",
    "\n",
    "# EDSR baseline as described in the EDSR paper (1.52M parameters)\n",
    "model_edsr = edsr(scale=4, num_res_blocks=16)\n",
    "\n",
    "# Adam optimizer with a scheduler that halfs learning rate after 200,000 steps\n",
    "optim_edsr = Adam(learning_rate=PiecewiseConstantDecay(boundaries=[200000], values=[1e-4, 5e-5]))\n",
    "\n",
    "# Compile and train model for 300,000 steps with L1 pixel loss\n",
    "model_edsr.compile(optimizer=optim_edsr, loss='mean_absolute_error')\n",
    " \n",
    "model_edsr.load_weights('weights-edsr-40E.h5')\n",
    "\n",
    "import imageio\n",
    "\n",
    "index = 1\n",
    "y1 = []\n",
    "for (x, y, w, h) in faces:\n",
    "    im = imageio.imread('roi' + str(index) + '.png')\n",
    "    y1.append(model_edsr.predict(tf.reshape(tf.convert_to_tensor(im), [1,w,h,3]),steps=1))\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "index = 0\n",
    "for (x,y,w,h) in faces:\n",
    "    PIL_image = Image.fromarray(np.asarray(y1[index]).reshape(4*w,4*h,3).astype('uint8'), 'RGB')\n",
    "    PIL_image.save('test' + str(index+1) + '.png')\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overlay big face on OG image: https://stackoverflow.com/questions/40895785/using-opencv-to-overlay-transparent-image-onto-another-image?fbclid=IwAR2FxAinlVh8GOKCZmQW0kZMOmAEt_caonwHTHPI3cDAiT3Yo-tJGTOmufs\n",
    "imageOG = cv2.imread(TEST_PHOTO)\n",
    "cv2.imwrite(\"combined.png\", imageOG)\n",
    "\n",
    "imageOG = cv2.imread(\"combined.png\")\n",
    "\n",
    "index = 1\n",
    "\n",
    "for (x, y, w, h) in faces:\n",
    "    bigHead = cv2.imread(\"test\" + str(index) + \".png\")\n",
    "    rows,cols,channels = bigHead.shape\n",
    "    imageOG[y-int((3/4)*cols):y-int((3/4)*cols)+cols, x-int((3/8)*rows):x-int((3/8)*rows)+rows] = bigHead #overlay\n",
    "    cv2.imwrite('combined.png', imageOG)\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cartoonify image and display\n",
    "#https://analyticsindiamag.com/converting-image-into-a-pencil-sketch-in-python/\n",
    "#OR\n",
    "#https://analyticsindiamag.com/converting-an-image-to-a-cartoon/?fbclid=IwAR1KsgK4sj3mpkUU-kIolQS1kUFnkkCo5bLDeq4BFPvfOzXRa-_gHF_PbWw\n",
    "\n",
    "combined = cv2.imread(\"combined.png\")\n",
    "img_gray = cv2.cvtColor(combined, cv2.COLOR_BGR2GRAY)\n",
    "img_invert = cv2.bitwise_not(img_gray)\n",
    "img_smoothing = cv2.GaussianBlur(img_invert, (31, 31),sigmaX=0, sigmaY=0)\n",
    "\n",
    "def dodgeV2(x, y):\n",
    "    return cv2.divide(x, 255 - y, scale=256)\n",
    "\n",
    "final_img = dodgeV2(img_gray, img_smoothing)\n",
    "\n",
    "cv2.imwrite('final.png', final_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/42594993/gradient-mask-blending-in-opencv-python\n",
    "def alphaBlend(img1, img2, mask):\n",
    "    \"\"\" alphaBlend img1 and img 2 (of CV_8UC3) with mask (CV_8UC1 or CV_8UC3)\n",
    "    \"\"\"\n",
    "    if mask.ndim==3 and mask.shape[-1] == 3:\n",
    "        alpha = mask/255.0\n",
    "    else:\n",
    "        alpha = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)/255.0\n",
    "    blended = cv2.convertScaleAbs(img1*(1-alpha) + img2*alpha)\n",
    "    return blended\n",
    "\n",
    "final = cv2.imread(\"final.png\")\n",
    "cv2.imwrite('finalmasked.png', final)\n",
    "\n",
    "index = 1\n",
    "for (x, y, w, h) in faces:\n",
    "    final = cv2.imread(\"finalmasked.png\")\n",
    "    bigHead = cv2.imread(\"test\" + str(index) + \".png\")\n",
    "    rows,cols,channels = bigHead.shape\n",
    "    H,W = final.shape[:2]\n",
    "    mask = np.zeros((H,W), np.uint8)\n",
    "    cv2.rectangle(mask, (x-int((3/8)*rows),y-int((3/4)*cols)), (x-int((3/8)*rows)+rows,y-int((3/4)*cols)+cols), (255,255,255), 8)\n",
    "    mask = cv2.GaussianBlur(mask, (21, 21),sigmaX=0, sigmaY=0)\n",
    "\n",
    "    blured = cv2.GaussianBlur(final, (31,31),sigmaX=0, sigmaY=0)\n",
    "\n",
    "    blended1 = alphaBlend(final, blured, mask)\n",
    "\n",
    "    cv2.imwrite('finalmasked.png', blended1)\n",
    "    index += 1\n",
    "    \n",
    "    #cv2.imshow(\"blened1\", blended1);\n",
    "    #cv2.imshow(\"mask\", mask);\n",
    "    #cv2.waitKey();cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OTHER ideas include changing what we big-ify, changing the background (to a touristy place) and the weather..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
